{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「MLProjects.ipynb」的副本",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87IENxVYUbxv",
        "outputId": "68af8e6e-5363-4ee2-ecc6-ff72a45b764e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFTwTYSaubzP",
        "outputId": "e8eb8bc2-5ea2-46de-f101-3821e53d3bd9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_excel('/content/drive/MyDrive/Colab/DryBeanDataset/Dry_Bean.xlsx')\n",
        "\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Area  Perimeter  MajorAxisLength  ...  ShapeFactor3  ShapeFactor4  Class\n",
            "0      28395    610.291       208.178117  ...      0.834222      0.998724      0\n",
            "1      28734    638.018       200.524796  ...      0.909851      0.998430      0\n",
            "2      29380    624.110       212.826130  ...      0.825871      0.999066      0\n",
            "3      30008    645.884       210.557999  ...      0.861794      0.994199      0\n",
            "4      30140    620.134       201.847882  ...      0.941900      0.999166      0\n",
            "...      ...        ...              ...  ...           ...           ...    ...\n",
            "13606  42097    759.696       288.721612  ...      0.642988      0.998385      6\n",
            "13607  42101    757.499       281.576392  ...      0.676099      0.998219      6\n",
            "13608  42139    759.321       281.539928  ...      0.676884      0.996767      6\n",
            "13609  42147    763.779       283.382636  ...      0.668237      0.995222      6\n",
            "13610  42159    772.237       295.142741  ...      0.616221      0.998180      6\n",
            "\n",
            "[13611 rows x 17 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5L2K8AGCJ48",
        "outputId": "97103c9d-59cd-4883-a789-add4d32ba88c"
      },
      "source": [
        "dataset = df.values\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.83950000e+04 6.10291000e+02 2.08178117e+02 ... 8.34222388e-01\n",
            "  9.98723889e-01 0.00000000e+00]\n",
            " [2.87340000e+04 6.38018000e+02 2.00524796e+02 ... 9.09850506e-01\n",
            "  9.98430331e-01 0.00000000e+00]\n",
            " [2.93800000e+04 6.24110000e+02 2.12826130e+02 ... 8.25870617e-01\n",
            "  9.99066137e-01 0.00000000e+00]\n",
            " ...\n",
            " [4.21390000e+04 7.59321000e+02 2.81539928e+02 ... 6.76884164e-01\n",
            "  9.96767264e-01 6.00000000e+00]\n",
            " [4.21470000e+04 7.63779000e+02 2.83382636e+02 ... 6.68236684e-01\n",
            "  9.95222420e-01 6.00000000e+00]\n",
            " [4.21590000e+04 7.72237000e+02 2.95142741e+02 ... 6.16220592e-01\n",
            "  9.98179623e-01 6.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ak0D5vrZtcq",
        "outputId": "5b522569-2ff6-4697-eb95-f97abfe4216b"
      },
      "source": [
        "X = dataset[:,0:16]\n",
        "X -= X.mean(axis=0)\n",
        "X /= X.std(axis=0)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.84074853 -1.1433189  -1.30659814 ...  2.40217287  1.92572347\n",
            "   0.83837103]\n",
            " [-0.82918764 -1.01392388 -1.39591111 ...  3.10089314  2.68970162\n",
            "   0.77113842]\n",
            " [-0.80715717 -1.07882906 -1.25235661 ...  2.23509147  1.84135576\n",
            "   0.91675514]\n",
            " ...\n",
            " [-0.37203825 -0.44783294 -0.45047814 ...  0.28920441  0.33632829\n",
            "   0.39025114]\n",
            " [-0.37176543 -0.42702856 -0.42897404 ...  0.22837538  0.2489734\n",
            "   0.03644001]\n",
            " [-0.37135619 -0.38755718 -0.2917356  ... -0.12777587 -0.2764814\n",
            "   0.71371948]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYxlyFhIgZyz",
        "outputId": "3349dfa5-3de7-4be2-bd7e-e11be6e08534"
      },
      "source": [
        "Y = dataset[:,16]\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 6. 6. 6.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d66iQ-Caj6S-",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "'''\n",
        "from sklearn import preprocessing\n",
        "\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_scale = min_max_scaler.fit_transform(X)\n",
        "print(X_scale)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "uE1uEPlglZkj",
        "outputId": "b391fb9e-1065-4a4e-b806-e7e011902e15"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# 選取自變數資料 \n",
        "#data = Y[:, :17] \n",
        "data = X\n",
        "\n",
        "# 選取因變數資料 \n",
        "#target = dataset['Class'] \n",
        "target = Y\n",
        "\n",
        "\n",
        "# 使用scikit-learn將資料集劃分為訓練集和測試集 \n",
        "train_data, test_data, train_target, test_target = train_test_split(data, target, test_size=0.2, train_size=0.8) \n",
        "\n",
        "\n",
        "'''\n",
        "X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.2)\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.2)\n",
        "#print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)\n",
        "print(X_val,Y_val)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nX_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, Y, test_size=0.2)\\nX_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.2)\\n#print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)\\nprint(X_val,Y_val)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Z2U1ZyLEr6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66edd85e-3674-4cc6-c41c-9c6d1744236d"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.python.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "x_train = train_data\n",
        "y_train = train_target\n",
        "x_test = test_data\n",
        "y_test = test_target\n",
        "\n",
        "'''\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_shape=(16,), activation=\"relu\"))\n",
        "model.add(Dense(16, activation=\"relu\"))\n",
        "model.add(Dense(7, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=20, batch_size=10, verbose=1)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(16, activation='relu'))\n",
        "\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "model.add(Dense(64, activation='relu'))\n",
        "\n",
        "\n",
        "\n",
        "#keras.regularizers.l1(0.01)\n",
        "#keras.regularizers.l2(0.01)\n",
        "#keras.regularizers.l1_l2(l2=0.01)\n",
        "#model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax',))\n",
        "#adm = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "optimizer='adam',\n",
        "metrics=['accuracy'])\n",
        "model.fit(x_train, y_train,\n",
        "epochs=25,\n",
        "batch_size=100)\n",
        "score = model.evaluate(x_test, y_test,batch_size=100)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "109/109 [==============================] - 3s 3ms/step - loss: 1.4612 - accuracy: 0.5564\n",
            "Epoch 2/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.4163 - accuracy: 0.8607\n",
            "Epoch 3/25\n",
            "109/109 [==============================] - 0s 2ms/step - loss: 0.2580 - accuracy: 0.9098\n",
            "Epoch 4/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9239\n",
            "Epoch 5/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9279\n",
            "Epoch 6/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1937 - accuracy: 0.9297\n",
            "Epoch 7/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1999 - accuracy: 0.9268\n",
            "Epoch 8/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.9276\n",
            "Epoch 9/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1944 - accuracy: 0.9270\n",
            "Epoch 10/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1898 - accuracy: 0.9327\n",
            "Epoch 11/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1948 - accuracy: 0.9287\n",
            "Epoch 12/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1897 - accuracy: 0.9310\n",
            "Epoch 13/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9296\n",
            "Epoch 14/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.9354\n",
            "Epoch 15/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9329\n",
            "Epoch 16/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1905 - accuracy: 0.9312\n",
            "Epoch 17/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1843 - accuracy: 0.9326\n",
            "Epoch 18/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9348\n",
            "Epoch 19/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9311\n",
            "Epoch 20/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1817 - accuracy: 0.9341\n",
            "Epoch 21/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1746 - accuracy: 0.9373\n",
            "Epoch 22/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1857 - accuracy: 0.9313\n",
            "Epoch 23/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.9313\n",
            "Epoch 24/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.9335\n",
            "Epoch 25/25\n",
            "109/109 [==============================] - 0s 3ms/step - loss: 0.1769 - accuracy: 0.9309\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.2074 - accuracy: 0.9254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQ9rIjR3RfL5",
        "outputId": "067344c5-9b2b-4678-bcaa-ae2c741b068e"
      },
      "source": [
        "loss, accuracy = model.evaluate(train_data, train_target)\n",
        "print(\"訓練資料集的準確度 = {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(test_data, test_target)\n",
        "print(\"測試資料集的準確度 = {:.4f}\".format(accuracy))\n",
        "print(\"_______\")\n",
        "print(train_test_split)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "341/341 [==============================] - 1s 2ms/step - loss: 0.1786 - accuracy: 0.9352\n",
            "訓練資料集的準確度 = 0.9352\n",
            "86/86 [==============================] - 0s 2ms/step - loss: 0.2074 - accuracy: 0.9254\n",
            "測試資料集的準確度 = 0.9254\n",
            "_______\n",
            "<function train_test_split at 0x7f1d3768a710>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fceqgxN2D-G_",
        "outputId": "b18fe2b6-1b80-4fec-bfc4-9c43092eac1c"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import scipy.stats as sci\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import ensemble, metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report \n",
        "\n",
        "forest = ensemble.RandomForestClassifier(n_estimators=1000)\n",
        "forest_fit = forest.fit(x_train, y_train)\n",
        "test_y_predicted = forest.predict(x_test)\n",
        "accuracy = metrics.accuracy_score(y_test, test_y_predicted)\n",
        "C = confusion_matrix(y_test, test_y_predicted)\n",
        "print(\"Score\",forest.score(x_test, y_test))\n",
        "print(accuracy)\n",
        "print(C)\n",
        "print(classification_report(y_test, test_y_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score 0.9203084832904884\n",
            "0.9203084832904884\n",
            "[[366   1   0   0   0   8   8]\n",
            " [  1 230   1  17   3   9   0]\n",
            " [  0   0  93   0   0   0   0]\n",
            " [  0  11   0 314   6   0   0]\n",
            " [  0   1   0   6 387   7   2]\n",
            " [  7   3   0   0  11 483  59]\n",
            " [ 12   0   0   0   2  42 633]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.96      0.95       383\n",
            "         1.0       0.93      0.88      0.91       261\n",
            "         2.0       0.99      1.00      0.99        93\n",
            "         3.0       0.93      0.95      0.94       331\n",
            "         4.0       0.95      0.96      0.95       403\n",
            "         5.0       0.88      0.86      0.87       563\n",
            "         6.0       0.90      0.92      0.91       689\n",
            "\n",
            "    accuracy                           0.92      2723\n",
            "   macro avg       0.93      0.93      0.93      2723\n",
            "weighted avg       0.92      0.92      0.92      2723\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiUeMGdV6JFZ",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "from sklearn import metrics \n",
        "\n",
        "train_predict = model.predict(train_data) \n",
        "test_predict = model.predict(test_data) \n",
        "\n",
        "train_proba = model.predict_proba(train_data)[:,6] \n",
        "test_proba = model.predict_proba(test_data)[:,6] \n",
        "\n",
        "#print(train_proba)\n",
        "\n",
        "print(metrics.confusion_matrix(test_target, test_predict, labels=[0,1,2,3,4,5,6])) \n",
        "print(metrics.classification_report(test_target, test_predict)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTm7UchTt_dU",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# For each class\n",
        "Y = label.classification(y, classes = [0,1,2,3,4,5,6])\n",
        "n_classes = Y.shape[1]\n",
        "precision = dict()\n",
        "recall = dict()\n",
        "average_precision = dict()\n",
        "for i in range(n_classes):\n",
        "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
        "                                                        y_score[:, i])\n",
        "    average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n",
        "\n",
        "# A \"micro-average\": quantifying score on all classes jointly\n",
        "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
        "    y_score.ravel())\n",
        "average_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n",
        "                                                     average=\"micro\")\n",
        "print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
        "      .format(average_precision[\"micro\"]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNALWQVOS0wM",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "# create confusion matrix\n",
        "y_true = np.array([-1]*700 + [0]*700 + [1]*700 + [2]*700 + [3]*700 + [4]*700 +[5]*700)\n",
        "y_pred = np.array([-1]*100 + [0]*100 + [1]*100 + [2]*100 + [3]*100 + [4]*100 + [5]*100 +\n",
        "            [-1]*100 + [0]*100 + [1]*100 + [2]*100 + [3]*100 + [4]*100 + [5]*100 +\n",
        "              [-1]*100 + [0]*100 + [1]*100 + [2]*100 + [3]*100 + [4]*100 + [5]*100 +\n",
        "                [-1]*100 + [0]*100 + [1]*100 + [2]*100 + [3]*100 + [4]*100 + [5]*100 +\n",
        "                  [-1]*100 + [0]*100 + [1]*100 + [2]*100 + [3]*100 + [4]*100 + [5]*100 +\n",
        "                    [-1]*100 + [0]*100 + [1]*100 + [2]*100 + [3]*100 + [4]*100 + [5]*100 +\n",
        "                      [-1]*100 + [0]*100 + [1]*100 + [2]*100 + [3]*100 + [4]*100 + [5]*100)\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "conf_matrix = pd.DataFrame(cm, index=[0,1,2,3,4,5,6], columns=[0,1,2,3,4,5,6])\n",
        "\n",
        "# plot size setting\n",
        "fig, ax = plt.subplots(figsize = (15,15))\n",
        "sns.heatmap(conf_matrix, annot=True, annot_kws={\"size\": 19}, cmap=\"Blues\")\n",
        "plt.ylabel('True label', fontsize=18)\n",
        "plt.xlabel('Predicted label', fontsize=18)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.savefig('confusion.pdf', bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V61yOdScS6k",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "print('------Weighted------')\n",
        "print('Weighted precision', precision_score(y_true, y_pred, average='weighted'))\n",
        "print('Weighted recall', recall_score(y_true, y_pred, average='weighted'))\n",
        "print('Weighted f1-score', f1_score(y_true, y_pred, average='weighted'))\n",
        "print('------Macro------')\n",
        "print('Macro precision', precision_score(y_true, y_pred, average='macro'))\n",
        "print('Macro recall', recall_score(y_true, y_pred, average='macro'))\n",
        "print('Macro f1-score', f1_score(y_true, y_pred, average='macro'))\n",
        "print('------Micro------')\n",
        "print('Micro precision', precision_score(y_true, y_pred, average='micro'))\n",
        "print('Micro recall', recall_score(y_true, y_pred, average='micro'))\n",
        "print('Micro f1-score', f1_score(y_true, y_pred, average='micro'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2PlJVIcfWpL",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "y_true=[0,1,2,3,4,5,6]\n",
        "y_pred=[0,1,2,3,4,5,6]\n",
        "\n",
        "f1 = f1_score( y_true, y_pred, average='macro' )\n",
        "p = precision_score(y_true, y_pred, average='macro')\n",
        "r = recall_score(y_true, y_pred, average='macro')\n",
        "\n",
        "print(f1, p, r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulqvE9pH6XUy"
      },
      "source": [
        "數值型"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOm_DW3T7sON"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#np.random.seed(7)  # 指定亂數種子"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C79mlf1AHs89",
        "outputId": "0df57d43-ede2-4a07-c6a4-03678dc49ce4"
      },
      "source": [
        "df = pd.read_excel('/content/drive/MyDrive/Colab/DryBeanDataset/Dry_Bean.xlsx')\n",
        "dataset = df\n",
        "#np.random.shuffle(dataset)  # 使用亂數打亂資料\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Area  Perimeter  MajorAxisLength  ...  ShapeFactor3  ShapeFactor4  Class\n",
            "0      28395    610.291       208.178117  ...      0.834222      0.998724      0\n",
            "1      28734    638.018       200.524796  ...      0.909851      0.998430      0\n",
            "2      29380    624.110       212.826130  ...      0.825871      0.999066      0\n",
            "3      30008    645.884       210.557999  ...      0.861794      0.994199      0\n",
            "4      30140    620.134       201.847882  ...      0.941900      0.999166      0\n",
            "...      ...        ...              ...  ...           ...           ...    ...\n",
            "13606  42097    759.696       288.721612  ...      0.642988      0.998385      6\n",
            "13607  42101    757.499       281.576392  ...      0.676099      0.998219      6\n",
            "13608  42139    759.321       281.539928  ...      0.676884      0.996767      6\n",
            "13609  42147    763.779       283.382636  ...      0.668237      0.995222      6\n",
            "13610  42159    772.237       295.142741  ...      0.616221      0.998180      6\n",
            "\n",
            "[13611 rows x 17 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCBVzVu1HzDw",
        "outputId": "4593a9dc-f6af-4726-fc22-07b10eac6c7f"
      },
      "source": [
        "X = dataset.iloc[:, 0:15]\n",
        "Y = dataset.iloc[:, 15]\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        Area  Perimeter  ...  ShapeFactor2  ShapeFactor3\n",
            "0      28395    610.291  ...      0.003147      0.834222\n",
            "1      28734    638.018  ...      0.003564      0.909851\n",
            "2      29380    624.110  ...      0.003048      0.825871\n",
            "3      30008    645.884  ...      0.003215      0.861794\n",
            "4      30140    620.134  ...      0.003665      0.941900\n",
            "...      ...        ...  ...           ...           ...\n",
            "13606  42097    759.696  ...      0.001749      0.642988\n",
            "13607  42101    757.499  ...      0.001886      0.676099\n",
            "13608  42139    759.321  ...      0.001888      0.676884\n",
            "13609  42147    763.779  ...      0.001852      0.668237\n",
            "13610  42159    772.237  ...      0.001640      0.616221\n",
            "\n",
            "[13611 rows x 15 columns]\n",
            "0        0.998724\n",
            "1        0.998430\n",
            "2        0.999066\n",
            "3        0.994199\n",
            "4        0.999166\n",
            "           ...   \n",
            "13606    0.998385\n",
            "13607    0.998219\n",
            "13608    0.996767\n",
            "13609    0.995222\n",
            "13610    0.998180\n",
            "Name: ShapeFactor4, Length: 13611, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H2TWTdGMOhZ"
      },
      "source": [
        "X -= X.mean(axis=0)\n",
        "X /= X.std(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlMpqhlDQW74"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# 選取自變數資料 \n",
        "\n",
        "data = X\n",
        "\n",
        "# 選取因變數資料 \n",
        "\n",
        "target = Y\n",
        "\n",
        "\n",
        "# 使用scikit-learn將資料集劃分為訓練集和測試集 \n",
        "train_data, test_data, train_target, test_target = train_test_split(data, target, test_size=0.2, train_size=0.8, random_state = 10) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtAPeWfI6zKZ",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "# 构建一个顺序模型\n",
        "model = Sequential()\n",
        "\n",
        "# 在模型中添加一个全连接层\n",
        "# 神经网络结构：1-10-1，即输入层为1个神经元，隐藏层10个神经元，输出层1个神经元。 \n",
        "\n",
        "# 激活函数加法1\n",
        "model.add(Dense(units=10, input_dim=1))\n",
        "model.add(activation = 'tanh')),\n",
        "model.add(Dense(units=1))\n",
        "model.add(activation = 'tanh')),\n",
        "\n",
        "# 激活函数加法2\n",
        "# model.add(Dense(units=10, input_dim=1, activation='relu'))\n",
        "# model.add(Dense(units=1, activation='relu'))\n",
        "\n",
        "# 定义优化算法\n",
        "sgd = SGD(lr=0.3)\n",
        "# sgd: Stochastic gradient descent,随机梯度下降法\n",
        "# mse: Mean Squared Error, 均方误差\n",
        "model.compile(optimizer=sgd, loss='mse')\n",
        "\n",
        "# 进行训练\n",
        "for step in range(3000):\n",
        "    # 每次训练一个批次\n",
        "    cost = model.train_on_batch(x_data, y_data)\n",
        "    # 每500个batch打印一次cost值\n",
        "    if step % 200 == 0:\n",
        "        print('cost: ', cost)\n",
        "# 打印权值和偏置值\n",
        "W, b = model.layers[0].get_weights()\n",
        "print('W：', W, ' b: ', b)\n",
        "# 模型有几层\n",
        "print(len(model.layers))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1ZR8jNVQgRn",
        "outputId": "cc76fbbd-4252-4d20-fb49-41669d4ee61a"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.python.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "\n",
        "model.add(Dense(32, activation='tanh'))\n",
        "\n",
        "model.add(Dense(64, activation='tanh'))\n",
        "\n",
        "model.add(Dense(128, activation='tanh'))\n",
        "\n",
        "\n",
        "#keras.regularizers.l1(0.01)\n",
        "#keras.regularizers.l2(0.01)\n",
        "#keras.regularizers.l1_l2(l2=0.01)\n",
        "\n",
        "model.add(Dense(1, activation='linear',))\n",
        "#adm = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "#adam= keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "model.compile(loss='mse',\n",
        "optimizer='adam')\n",
        "metrics=[\"mae\"])\n",
        "'''\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation=\"tanh\"))\n",
        "model.add(Dense(32, activation=\"tanh\"))\n",
        "model.add(Dense(1))\n",
        "# 編譯模型\n",
        "model.compile(loss=\"mape\", optimizer=\"adam\",\n",
        "metrics=[\"mape\"])\n",
        "\n",
        "mode2 = Sequential()\n",
        "model.add(Dense(32, activation=\"tanh\"))\n",
        "model.add(Dense(32, activation=\"tanh\"))\n",
        "model.add(Dense(1))\n",
        "# 編譯模型\n",
        "mode2.compile(loss=\"mse\", optimizer=\"adam\",\n",
        "metrics=[\"mse\"])\n",
        "\n",
        "\n",
        "\n",
        "model.fit(train_data, train_target,\n",
        "epochs=10,\n",
        "batch_size=10)\n",
        "score = model.evaluate(test_data, test_target,batch_size=100)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "mode2.fit(train_data, train_target,\n",
        "epochs=10,\n",
        "batch_size=10)\n",
        "score = model.evaluate(test_data, test_target,batch_size=100)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1089/1089 [==============================] - 4s 3ms/step - loss: 8.8279 - mape: 8.8279\n",
            "Epoch 2/10\n",
            "1089/1089 [==============================] - 3s 3ms/step - loss: 0.4011 - mape: 0.4011\n",
            "Epoch 3/10\n",
            "1089/1089 [==============================] - 3s 3ms/step - loss: 0.3710 - mape: 0.3710\n",
            "Epoch 4/10\n",
            "1089/1089 [==============================] - 3s 3ms/step - loss: 0.4316 - mape: 0.4316\n",
            "Epoch 5/10\n",
            "1089/1089 [==============================] - 3s 3ms/step - loss: 0.3772 - mape: 0.3772\n",
            "Epoch 6/10\n",
            "1089/1089 [==============================] - 3s 3ms/step - loss: 0.3545 - mape: 0.3545\n",
            "Epoch 7/10\n",
            "1089/1089 [==============================] - 3s 3ms/step - loss: 0.4013 - mape: 0.4013\n",
            "Epoch 8/10\n",
            "1089/1089 [==============================] - 3s 3ms/step - loss: 0.4190 - mape: 0.4190\n",
            "Epoch 9/10\n",
            "1089/1089 [==============================] - 3s 3ms/step - loss: 0.3397 - mape: 0.3397\n",
            "Epoch 10/10\n",
            "1089/1089 [==============================] - 3s 3ms/step - loss: 0.3520 - mape: 0.3520\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3818 - mape: 0.3818\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 32)                512       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 33        \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                64        \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,754\n",
            "Trainable params: 2,754\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1089/1089 [==============================] - 2s 2ms/step - loss: 1.9739 - mse: 1.9739\n",
            "Epoch 2/10\n",
            "1089/1089 [==============================] - 2s 2ms/step - loss: 1.9931 - mse: 1.9931\n",
            "Epoch 3/10\n",
            "1089/1089 [==============================] - 2s 2ms/step - loss: 1.9887 - mse: 1.9887\n",
            "Epoch 4/10\n",
            "1089/1089 [==============================] - 2s 2ms/step - loss: 1.9774 - mse: 1.9774\n",
            "Epoch 5/10\n",
            "1089/1089 [==============================] - 2s 2ms/step - loss: 1.9884 - mse: 1.9884\n",
            "Epoch 6/10\n",
            "1089/1089 [==============================] - 2s 2ms/step - loss: 1.9879 - mse: 1.9879\n",
            "Epoch 7/10\n",
            "1089/1089 [==============================] - 2s 2ms/step - loss: 1.9821 - mse: 1.9821\n",
            "Epoch 8/10\n",
            "1089/1089 [==============================] - 2s 2ms/step - loss: 1.9834 - mse: 1.9834\n",
            "Epoch 9/10\n",
            "1089/1089 [==============================] - 2s 2ms/step - loss: 1.9983 - mse: 1.9983\n",
            "Epoch 10/10\n",
            "1089/1089 [==============================] - 2s 2ms/step - loss: 1.9866 - mse: 1.9866\n",
            "28/28 [==============================] - 0s 2ms/step - loss: 0.3818 - mape: 0.3818\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 32)                512       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 33        \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                64        \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 2,754\n",
            "Trainable params: 2,754\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE4-WoT2QjNt",
        "outputId": "ab815e77-f13f-43cc-be99-4e9a5b913454"
      },
      "source": [
        "import math\n",
        "mape = model.evaluate(train_data, train_target)\n",
        "print(\"MAPE: \", mape)\n",
        "\n",
        "mse = mode2.evaluate(train_data, train_target)\n",
        "print(\"MSE: \", mse)\n",
        "\n",
        "r2 = mode2.evaluate(train_data, train_target)\n",
        "np_r2 = np.array(r2)\n",
        "rmse = np_r2**0.5\n",
        "print(\"RMSE: \", rmse)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "341/341 [==============================] - 1s 2ms/step - loss: 0.3759 - mape: 0.3759\n",
            "MAPE:  [0.37589237093925476, 0.37589237093925476]\n",
            "341/341 [==============================] - 1s 2ms/step - loss: 1.9855 - mse: 1.9855\n",
            "MSE:  [1.9854813814163208, 1.9854813814163208]\n",
            "341/341 [==============================] - 1s 2ms/step - loss: 1.9855 - mse: 1.9855\n",
            "RMSE:  [1.40907111 1.40907111]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnqjnD5sLlCI"
      },
      "source": [
        "Adult Data Sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCOQDbkD5FN2"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "%load_ext tensorboard\n",
        "# ! pip install tensorflow-addons\n",
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.python.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5aSJ9Eq4zvY"
      },
      "source": [
        "Train = pd.read_csv('/content/drive/MyDrive/Colab/DryBeanDataset/Train.csv')\n",
        "Test = pd.read_csv('/content/drive/MyDrive/Colab/DryBeanDataset/Test.csv')\n",
        "#df = pd.read_csv('/content/drive/MyDrive/Colab/DryBeanDataset/adult.csv')\n",
        "Train_dataset = Train\n",
        "Test_dataset = Test\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNO0atVImkxM"
      },
      "source": [
        "encoder = LabelEncoder()\n",
        "Train_features = Train_dataset.columns.tolist()\n",
        "for each in Train_features:\n",
        "    Train_dataset[each] = encoder.fit_transform(Train_dataset[each])\n",
        "\n",
        "\n",
        "Test_features = Test_dataset.columns.tolist()\n",
        "for col in Test_features:\n",
        "    Test_dataset[col] = encoder.fit_transform(Test_dataset[col])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXQSbFY2mxfv"
      },
      "source": [
        "x_train = Train_dataset.iloc[:, 1:16].values  \n",
        "\n",
        "y_train = Train_dataset.iloc[:, 0].values\n",
        "\n",
        "x_test = Test_dataset.iloc[:, 1:16].values  \n",
        "\n",
        "y_test = Test_dataset.iloc[:, 0].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzqGCIhvm2Du",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#encoder = LabelEncoder()\n",
        "#encoder.fit(y_train)\n",
        "#encoded_y_train = encoder.transform(y_train)\n",
        "#dummy_yt = np_utils.to_categorical(encoded_y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWAxLmZam4dp",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#encoder = LabelEncoder()\n",
        "#encoder.fit(y_test)\n",
        "#encoded_y_test = encoder.transform(y_test)\n",
        "#dummy_y = np_utils.to_categorical(encoded_y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS7Ag2Xgr_WS",
        "outputId": "9ccd2617-caa3-4ff3-f900-4e4d82e7321d"
      },
      "source": [
        "print(x_train, x_test, y_train, y_test)\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   22     7  2671 ...    39    39     0]\n",
            " [   33     6  2926 ...    12    39     0]\n",
            " [   21     4 14086 ...    39    39     0]\n",
            " ...\n",
            " [   41     4  7883 ...    39    39     0]\n",
            " [    5     4 12881 ...    19    39     0]\n",
            " [   35     5 17825 ...    39    39     1]] [[    8     4  8931 ...    39    38     0]\n",
            " [   21     4  1888 ...    49    38     0]\n",
            " [   11     2 11540 ...    39    38     1]\n",
            " ...\n",
            " [   21     4 12014 ...    49    38     0]\n",
            " [   27     4  1718 ...    39    38     0]\n",
            " [   18     5  6520 ...    59    38     1]] [ 9  9 11 ... 11 11 11] [ 1 11  7 ...  9  9  9]\n",
            "(32561, 14) (16281, 14) (32561,) (16281,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVa2bBizm_tC"
      },
      "source": [
        "import keras \n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense,Dropout,Activation,Flatten\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras import initializers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CAXfAVGRnHn4",
        "outputId": "59d69131-36a7-4243-9acf-2c2d8f2122ae"
      },
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.python.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1024, activation=\"relu\"))\n",
        "model.add(Dense(2048, activation=\"relu\"))\n",
        "\n",
        "model.add(Dense(16, activation= 'softmax'))\n",
        "# 編譯模型\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\",\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "epochs=30,\n",
        "batch_size=50)\n",
        "score = model.evaluate(x_test, y_test,batch_size=10)\n",
        "model.summary()\n",
        "'''\n",
        "classifier = Sequential()\n",
        "\n",
        "# Adding the input layer and the first hidden layer\n",
        "\n",
        "classifier.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu', input_dim = 14))\n",
        "\n",
        "\n",
        "# Adding the second hidden layer\n",
        "\n",
        "classifier.add(Dense(units = 30, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "# Adding the output layer\n",
        "\n",
        "classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'softmax'))\n",
        "\n",
        "\n",
        "# Compiling the ANN\n",
        "\n",
        "classifier.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "classifier.fit(x_train, y_train, batch_size = 50, epochs = 30)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "652/652 [==============================] - 3s 5ms/step - loss: 382.6369 - accuracy: 0.1945\n",
            "Epoch 2/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 2.7431 - accuracy: 0.2647\n",
            "Epoch 3/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 2.2268 - accuracy: 0.2880\n",
            "Epoch 4/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 2.1824 - accuracy: 0.2866\n",
            "Epoch 5/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 2.0502 - accuracy: 0.3278\n",
            "Epoch 6/30\n",
            "652/652 [==============================] - 3s 5ms/step - loss: 2.0012 - accuracy: 0.3401\n",
            "Epoch 7/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 2.1020 - accuracy: 0.3263\n",
            "Epoch 8/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 2.0680 - accuracy: 0.3293\n",
            "Epoch 9/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 1.9638 - accuracy: 0.3407\n",
            "Epoch 10/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 1.9224 - accuracy: 0.3548\n",
            "Epoch 11/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 1.8816 - accuracy: 0.3756\n",
            "Epoch 12/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 1.6746 - accuracy: 0.4378\n",
            "Epoch 13/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 1.3540 - accuracy: 0.5309\n",
            "Epoch 14/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 1.2148 - accuracy: 0.5517\n",
            "Epoch 15/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 1.0590 - accuracy: 0.6072\n",
            "Epoch 16/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.9400 - accuracy: 0.6480\n",
            "Epoch 17/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.8954 - accuracy: 0.6747\n",
            "Epoch 18/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.7575 - accuracy: 0.7206\n",
            "Epoch 19/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 1.0208 - accuracy: 0.6282\n",
            "Epoch 20/30\n",
            "652/652 [==============================] - 3s 5ms/step - loss: 0.6596 - accuracy: 0.7561\n",
            "Epoch 21/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.6589 - accuracy: 0.7518\n",
            "Epoch 22/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.5347 - accuracy: 0.8141\n",
            "Epoch 23/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.6160 - accuracy: 0.7876\n",
            "Epoch 24/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.7212 - accuracy: 0.7300\n",
            "Epoch 25/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.8173 - accuracy: 0.7148\n",
            "Epoch 26/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.8627 - accuracy: 0.6822\n",
            "Epoch 27/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.7269 - accuracy: 0.7375\n",
            "Epoch 28/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.7189 - accuracy: 0.7309\n",
            "Epoch 29/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.5720 - accuracy: 0.7919\n",
            "Epoch 30/30\n",
            "652/652 [==============================] - 3s 4ms/step - loss: 0.5347 - accuracy: 0.7981\n",
            "1629/1629 [==============================] - 5s 3ms/step - loss: 0.3452 - accuracy: 0.9083\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_37 (Dense)             (None, 1024)              15360     \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 2048)              2099200   \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 16)                32784     \n",
            "=================================================================\n",
            "Total params: 2,147,344\n",
            "Trainable params: 2,147,344\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nclassifier = Sequential()\\n\\n# Adding the input layer and the first hidden layer\\n\\nclassifier.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu', input_dim = 14))\\n\\n\\n# Adding the second hidden layer\\n\\nclassifier.add(Dense(units = 30, kernel_initializer = 'uniform', activation = 'relu'))\\n\\n# Adding the output layer\\n\\nclassifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'softmax'))\\n\\n\\n# Compiling the ANN\\n\\nclassifier.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\\n\\nclassifier.fit(x_train, y_train, batch_size = 50, epochs = 30)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhHIW-dTvDbP",
        "outputId": "e0b9bf5a-4796-4bbf-88e9-4b0c2b295b2d"
      },
      "source": [
        "  loss, accuracy = model.evaluate(x_train, y_train)\n",
        "print(\"訓練資料集的準確度 = {:.4f}\".format(accuracy))\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(\"測試資料集的準確度 = {:.4f}\".format(accuracy))\n",
        "print(\"_______\")\n",
        "print(train_test_split)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1018/1018 [==============================] - 3s 3ms/step - loss: 0.4176 - accuracy: 0.8449\n",
            "訓練資料集的準確度 = 0.8449\n",
            "509/509 [==============================] - 1s 3ms/step - loss: 0.3452 - accuracy: 0.9083\n",
            "測試資料集的準確度 = 0.9083\n",
            "_______\n",
            "<function train_test_split at 0x7f1d3768a710>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r89M7TXEv-F8",
        "outputId": "c1a73d40-69cb-4421-bcce-d1648ddd2cfa"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import scipy.stats as sci\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import ensemble, metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report \n",
        "\n",
        "forest = ensemble.RandomForestClassifier(n_estimators=1000)\n",
        "forest_fit = forest.fit(x_train, y_train)\n",
        "test_y_predicted = forest.predict(x_test)\n",
        "accuracy = metrics.accuracy_score(y_test, test_y_predicted)\n",
        "C = confusion_matrix(y_test, test_y_predicted)\n",
        "print(\"Score\",forest.score(x_test, y_test))\n",
        "print(accuracy)\n",
        "print(C)\n",
        "print(classification_report(y_test, test_y_predicted))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score 0.9924451814999079\n",
            "0.9924451814999079\n",
            "[[ 455    0    0    0    0    0    1    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   0  637    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   0    5  219    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   0    0    0   59    7   13    0    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    0    0    0  155   19    1    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   1    0    0    0    2  303    3    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   3    0    0    0    0    1  238    0    0    0    0    0    0    0\n",
            "     0    0]\n",
            " [   0    0    0    0    0    0    0  532    1    1    0    0    0    0\n",
            "     0    0]\n",
            " [   0    0    0    0    0    0    0    0  679    0    0    0    0    0\n",
            "     0    0]\n",
            " [   0    0    0    0    0    0    0    0    0 2670    0    0    0    0\n",
            "     0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    5  162    0    8    0\n",
            "     6    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0 5283    0    0\n",
            "     0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    5    0    0  929    0\n",
            "     0    0]\n",
            " [   0    0    0    7    6    6    1    0    0    0    0    0    0   12\n",
            "     0    0]\n",
            " [   0    0    0    0    0    0    0    0    0    2    1    0   17    0\n",
            "   238    0]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0 3587]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       456\n",
            "           1       0.99      1.00      1.00       637\n",
            "           2       1.00      0.98      0.99       224\n",
            "           3       0.89      0.75      0.81        79\n",
            "           4       0.91      0.88      0.90       176\n",
            "           5       0.89      0.98      0.93       309\n",
            "           6       0.98      0.98      0.98       242\n",
            "           7       1.00      1.00      1.00       534\n",
            "           8       1.00      1.00      1.00       679\n",
            "           9       1.00      1.00      1.00      2670\n",
            "          10       0.99      0.90      0.94       181\n",
            "          11       1.00      1.00      1.00      5283\n",
            "          12       0.97      0.99      0.98       934\n",
            "          13       1.00      0.38      0.55        32\n",
            "          14       0.98      0.92      0.95       258\n",
            "          15       1.00      1.00      1.00      3587\n",
            "\n",
            "    accuracy                           0.99     16281\n",
            "   macro avg       0.97      0.92      0.94     16281\n",
            "weighted avg       0.99      0.99      0.99     16281\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTIl955p4__U",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "dataset = df.replace('?', np.nan)\n",
        "dataset[pd.isnull(df).any(axis=1)].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-ZZ3YTX6Ajm",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "df.dropna(inplace=True)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H6OXVMZ6CHC",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "df.drop('educational-num', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-fg5IXu6Knx",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "categorical_columns = ['workclass','income','marital-status','occupation','relationship','gender','native-country','race']\n",
        "label_column = ['education']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1xzRlzp6r2V",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "def show_unique_values(columns):\n",
        "  for column in columns:\n",
        "    uniq = df[column].unique().tolist()\n",
        "    print(column+ \" has \"+ str(len(uniq)) +\" values\" + \" : \" + str(uniq))\n",
        "show_unique_values(categorical_columns)\n",
        "show_unique_values(label_column)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2k9C4s36tUt",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "def convert_to_int(columns):\n",
        "  for column in columns:\n",
        "    unique_values = df[column].unique().tolist()\n",
        "    dic = {}\n",
        "    for indx, val in enumerate(unique_values):\n",
        "      dic[val]=indx\n",
        "    df[column] = df[column].map(dic).astype(int)\n",
        "    print(column + \" done!\")\n",
        "convert_to_int(label_column)\n",
        "convert_to_int(categorical_columns)\n",
        "show_unique_values(label_column)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoI0Ah9jGTj6",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "def convert_to_onehot(data,columns):\n",
        "  dummies = pd.get_dummies(data[columns])\n",
        "  data = data.drop(columns, axis=1)\n",
        "  data = pd.concat([data, dummies], axis=1)\n",
        "  return data\n",
        "\n",
        "df = convert_to_onehot(df,categorical_columns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1mECI476zwe",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xIdf1yo8jiG",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "normalize_columns = ['age', 'fnlwgt', 'capital-gain','capital-loss','hours-per-week']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def show_values(columns):\n",
        "  for column in columns:\n",
        "    max_val = df[column].max()\n",
        "    min_val = df[column].min()\n",
        "    mean_val = df[column].mean()\n",
        "    var_val = df[column].var()\n",
        "    print(column +': values=['+str(min_val)+','+str(max_val)+'] , mean='+str(mean_val)+' , var='+str(var_val))\n",
        "\n",
        "show_values(normalize_columns)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXpskuPD8xN_",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "df_1 = df\n",
        "\n",
        "def normalize(columns):\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  df[columns] = scaler.fit_transform(df[columns])\n",
        "normalize(normalize_columns)\n",
        "show_values(normalize_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCvR0yJ19J7W",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "som4uEvy9ME1",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_data = df.drop('education',axis=1)\n",
        "y_labels = df['education']\n",
        "'''\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_data, y_labels, test_size=0.2,shuffle=True)\n",
        "# X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,y_train_full,test_size=0.2,shuffle=True)\n",
        "'''\n",
        "\n",
        "train_data, test_data, train_target, test_target = train_test_split(x_data, y_labels, train_size=0.8, test_size = 0.2) \n",
        "\n",
        "print(train_data,test_data,train_target,test_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YILIJGIoHHHI",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "from keras import backend as K\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "        \n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTyeXnDXM_Pp",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "#print(train_data.shape[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3I2Ke8uNCCs",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "'''NO'''\n",
        "import numpy as np\n",
        "from keras.models import Sequential,Input,Model\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.utils import np_utils\n",
        "from tensorflow.python.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(16, activation=\"relu\"))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dense(204, activation=\"relu\"))\n",
        "model.add(Dense(16, activation= 'softmax'))\n",
        "# 編譯模型\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=\"adam\",\n",
        "metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "model.fit(train_data, train_target,\n",
        "epochs=10,\n",
        "batch_size=10\n",
        "score = model.evaluate(test_data, test_target,batch_size=10)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaMk37NnRcKd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}